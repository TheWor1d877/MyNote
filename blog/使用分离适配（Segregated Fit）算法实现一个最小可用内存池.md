> 你是否曾好奇，程序中的`malloc()`和`new`背后究竟发生了什么？
> 为什么频繁的小内存分配会拖慢程序？
> 内存碎片又是如何产生的？

## 一个令人震惊的事实
这两个操作的开销可能相差100倍！
```c
int* p1 = (int*)malloc(16);  // 系统调用，开销大
int* p2 = memory_pool_alloc(16);  // 内存池分配，速度快
```
原因就在于系统调用的代价：
- malloc: 用户态→内核态切换→查找空闲内存→返回
- 内存池: 直接返回预分配的内存块
***如果我们先让程序分配到合适的内存，就可以省去每一次new与delete带来的系统开销！***


## 内存碎片：看不见的性能杀手
想象一下这样的场景：你的程序总共需要24KB内存，而系统显示有24KB空闲内存，但当你尝试分配一个12KB的块时，却被告知“内存不足”。这不是系统在开玩笑，而是**内存碎片**在作祟——一种看似有足够内存，却无法有效使用的尴尬局面。

问题的根源在于内存分配的“连续性”要求。当程序请求12KB连续内存时，系统不能把两个不相邻的空闲块拼凑起来给你，就像你不能把北京和上海的两套小公寓合并成一套大房子一样。

![[Attachments/Pasted image 20251215163148.png]]

## 主流内存分配算法对比

| 算法       | 时间复杂度     | 内存碎片 | 适用场景     |
| -------- | --------- | ---- | -------- |
| **首次适应** | O(n)      | 较多   | 学习入门     |
| **最佳适应** | O(n)      | 严重   | 内存紧张系统   |
| **循环首次** | O(n)      | 一般   | 中等负载     |
| **伙伴系统** | O(log n)  | 较少   | Linux内核  |
| **分离适配** | O(1)~O(n) | 较少   | **生产环境** |

## 什么是分离适配？
一句话：分离适配就是把内存块按大小分成不同的类别，每个类别单独管理。
如下:
```
内存池
├── 小型内存区 (8-64字节)
│   ├── 8字节链表: ○→○→○
│   ├── 16字节链表: ○→○
│   └── 32字节链表: ○→○→○→○
│
├── 中型内存区 (64-256字节)
│   ├── 64字节链表: ○→○
│   ├── 128字节链表: ○
│   └── 256字节链表: ○→○
│
└── 大型内存区 (>256字节)
    └── 普通链表: ○→○→○
```

## 为什么要分离适配？
#### 普通链表的缺点
只有一个空闲链表时，所有大小的块都在这里，假如分配32字节，需要遍历整个链表
能找到，但是可能遍历了很多大块
```cpp
MemoryBlock* free_list;  // 所有大小的块都在这里

while (block) {
    if (block->size >= 32 && block->is_free) {
        return block;
    }
    block = block->next;
}
```

#### 分离链表
按照字节数量，分配链表，建立多个链表
查找的时候，根据字节数量去相应的链表中寻找
```cpp
// 多个空闲链表，按大小分类
MemoryBlock* small_blocks;   // 8-64字节
MemoryBlock* medium_blocks;  // 64-256字节  
MemoryBlock* large_blocks;   // 256+字节

// 分配32字节时，直接去small_blocks找
// 分配500字节时，直接去large_blocks找
```

他的优势就在于以下三个方面:
1. 小内存请求直达专属链表，无需遍历, O(1)极速分配。
2. 按大小隔离存储，避免大小块混杂，实现智能碎片管理。
3. 核心仅需链表数组+大小映射，代码清晰易维护，实现起来简单。

## 完整代码实现

## 性能对比测试：与标准malloc的性能较量

## 内存池三大进阶优化
1. 线程本地存储（TLS）
解决核心问题：多线程竞争锁导致的性能瓶颈。
	传统内存池需要全局锁保护，多个线程同时分配内存时会相互阻塞。线程本地存储为每个线程创建独立的内存池实例，完全消除锁竞争。
2. 预分配与缓存
解决核心问题：首次分配时的冷启动延迟。
	程序启动或遇到新分配模式时，内存池需要初始化数据结构，这会造成首次分配较慢。预热策略提前准备常用内存块。
3. 内存对齐优化
解决核心问题：未对齐内存访问的性能损失。
	现代CPU和SIMD指令要求内存地址按特定边界对齐，未对齐访问会导致性能下降甚至崩溃。


